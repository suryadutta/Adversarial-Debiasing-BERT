{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "src_path =   '../src' # change as needed\n",
    "sys.path.insert(0,src_path)\n",
    "\n",
    "import numpy as np\n",
    "import data_generator\n",
    "import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 3007.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tag  cat  occurences\n",
      "0    B-MISC    0           8\n",
      "1     I-LOC    1        2563\n",
      "2    I-MISC    2        1106\n",
      "3     I-ORG    3        1871\n",
      "4     I-PER    4        8216\n",
      "5         O    5       53719\n",
      "6  [nerCLS]    6        3000\n",
      "7  [nerPAD]    7      293949\n",
      "8  [nerSEP]    8        3000\n",
      "9    [nerX]    9       16568\n",
      "\n",
      "                tag  cat  occurences\n",
      "0  AFRICAN-AMERICAN    0        3382\n",
      "1          EUROPEAN    1        1555\n",
      "2         [raceCLS]    2        3000\n",
      "3         [racePAD]    3      293949\n",
      "4         [raceSEP]    4        3000\n",
      "5           [raceX]    5       79114\n",
      "\n",
      "           tag  cat  occurences\n",
      "0       FEMALE    0        2855\n",
      "1         MALE    1        2082\n",
      "2  [genderCLS]    2        3000\n",
      "3  [genderPAD]    3      293949\n",
      "4  [genderSEP]    4        3000\n",
      "5    [genderX]    5       79114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Start session\n",
    "max_length = 128\n",
    "\n",
    "train_data, val_data, test_data = data_generator.GetData(max_length, sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def getDebiasedModel(max_input_length, train_layers):\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_input_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_input_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_input_length,), name=\"segment_ids\")\n",
    "\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_sequence = model_utils.BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='pred_dense')(bert_sequence)\n",
    "\n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "\n",
    "    pred = tf.keras.layers.Dense(10, activation='softmax', name='ner')(dense)\n",
    "    \n",
    "    genderPred = tf.keras.layers.Dense(6, activation='softmax', name='gender')(pred)\n",
    "\n",
    "    racePred = tf.keras.layers.Dense(6, activation='softmax', name='race')(pred)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs={\n",
    "        \"ner\": pred,\n",
    "        \"race\": racePred,\n",
    "        \"gender\": genderPred\n",
    "    })\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(data, batch_size=32):\n",
    "    idx = np.random.randint(len(data[\"nerLabels\"]), size=batch_size)\n",
    "    return [\n",
    "        data[\"inputs\"][0][idx], \n",
    "        data[\"inputs\"][1][idx], \n",
    "        data[\"inputs\"][2][idx], \n",
    "        data[\"nerLabels\"][idx],\n",
    "        data[\"genderLabels\"][idx],\n",
    "        data[\"raceLabels\"][idx]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pred_dense (Dense)              (None, None, 256)    196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           pred_dense[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 10)     2570        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gender (Dense)                  (None, None, 6)      66          ner[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "race (Dense)                    (None, None, 6)      66          ner[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 109,130,962\n",
      "Trainable params: 7,287,438\n",
      "Non-trainable params: 101,843,524\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = getDebiasedModel(max_length, 1)\n",
    "\n",
    "pred_learning_rate = 2**-16\n",
    "protect_learning_rate = 2**-16\n",
    "num_epochs = 5\n",
    "\n",
    "num_train_samples = len(train_data[\"nerLabels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "protect_loss_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_normalize(x):\n",
    "    \"\"\"Returns the input vector, normalized.\n",
    "\n",
    "    A small number is added to the norm so that this function does not break when\n",
    "    dealing with the zero vector (e.g. if the weights are zero-initialized).\n",
    "\n",
    "    Args:\n",
    "    x: the tensor to normalize\n",
    "    \"\"\"\n",
    "    return x / (tf.norm(x) + np.finfo(np.float32).tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 2.678376; batch adversarial loss: 3.734007\n",
      "epoch 0; iter: 1; batch classifier loss: 2.585400; batch adversarial loss: 3.721679\n",
      "epoch 0; iter: 2; batch classifier loss: 2.546495; batch adversarial loss: 3.730783\n",
      "epoch 0; iter: 3; batch classifier loss: 2.499578; batch adversarial loss: 3.728687\n",
      "epoch 0; iter: 4; batch classifier loss: 2.417089; batch adversarial loss: 3.737558\n",
      "epoch 0; iter: 5; batch classifier loss: 2.289476; batch adversarial loss: 3.731979\n",
      "epoch 0; iter: 6; batch classifier loss: 2.252623; batch adversarial loss: 3.734449\n",
      "epoch 0; iter: 7; batch classifier loss: 2.161414; batch adversarial loss: 3.727175\n",
      "epoch 0; iter: 8; batch classifier loss: 2.166079; batch adversarial loss: 3.722918\n",
      "epoch 0; iter: 9; batch classifier loss: 2.101038; batch adversarial loss: 3.724814\n",
      "epoch 0; iter: 10; batch classifier loss: 2.035396; batch adversarial loss: 3.725134\n",
      "epoch 0; iter: 11; batch classifier loss: 1.998918; batch adversarial loss: 3.726599\n",
      "epoch 0; iter: 12; batch classifier loss: 1.931057; batch adversarial loss: 3.723634\n",
      "epoch 0; iter: 13; batch classifier loss: 1.819195; batch adversarial loss: 3.734349\n",
      "epoch 0; iter: 14; batch classifier loss: 1.808850; batch adversarial loss: 3.730552\n",
      "epoch 0; iter: 15; batch classifier loss: 1.762997; batch adversarial loss: 3.734618\n",
      "epoch 0; iter: 16; batch classifier loss: 1.707420; batch adversarial loss: 3.734702\n",
      "epoch 0; iter: 17; batch classifier loss: 1.643114; batch adversarial loss: 3.716355\n",
      "epoch 0; iter: 18; batch classifier loss: 1.570874; batch adversarial loss: 3.739947\n",
      "epoch 0; iter: 19; batch classifier loss: 1.546776; batch adversarial loss: 3.720227\n",
      "epoch 0; iter: 20; batch classifier loss: 1.488895; batch adversarial loss: 3.728689\n",
      "epoch 0; iter: 21; batch classifier loss: 1.446590; batch adversarial loss: 3.730140\n",
      "epoch 0; iter: 22; batch classifier loss: 1.414689; batch adversarial loss: 3.727072\n",
      "epoch 0; iter: 23; batch classifier loss: 1.382561; batch adversarial loss: 3.728362\n",
      "epoch 0; iter: 24; batch classifier loss: 1.364309; batch adversarial loss: 3.729270\n",
      "epoch 0; iter: 25; batch classifier loss: 1.285569; batch adversarial loss: 3.721978\n",
      "epoch 0; iter: 26; batch classifier loss: 1.275097; batch adversarial loss: 3.718330\n",
      "epoch 0; iter: 27; batch classifier loss: 1.234471; batch adversarial loss: 3.723352\n",
      "epoch 0; iter: 28; batch classifier loss: 1.148277; batch adversarial loss: 3.728103\n",
      "epoch 0; iter: 29; batch classifier loss: 1.121372; batch adversarial loss: 3.718870\n",
      "epoch 0; iter: 30; batch classifier loss: 1.128097; batch adversarial loss: 3.732084\n",
      "epoch 1; iter: 0; batch classifier loss: 1.036381; batch adversarial loss: 3.732941\n",
      "epoch 1; iter: 1; batch classifier loss: 1.042026; batch adversarial loss: 3.727338\n",
      "epoch 1; iter: 2; batch classifier loss: 0.952354; batch adversarial loss: 3.720267\n",
      "epoch 1; iter: 3; batch classifier loss: 0.996197; batch adversarial loss: 3.724107\n",
      "epoch 1; iter: 4; batch classifier loss: 0.931898; batch adversarial loss: 3.728052\n",
      "epoch 1; iter: 5; batch classifier loss: 0.954432; batch adversarial loss: 3.721893\n",
      "epoch 1; iter: 6; batch classifier loss: 0.950264; batch adversarial loss: 3.719858\n",
      "epoch 1; iter: 7; batch classifier loss: 0.868991; batch adversarial loss: 3.719575\n",
      "epoch 1; iter: 8; batch classifier loss: 0.874406; batch adversarial loss: 3.732162\n",
      "epoch 1; iter: 9; batch classifier loss: 0.846509; batch adversarial loss: 3.721153\n",
      "epoch 1; iter: 10; batch classifier loss: 0.844095; batch adversarial loss: 3.712003\n",
      "epoch 1; iter: 11; batch classifier loss: 0.810338; batch adversarial loss: 3.724535\n",
      "epoch 1; iter: 12; batch classifier loss: 0.783894; batch adversarial loss: 3.725021\n",
      "epoch 1; iter: 13; batch classifier loss: 0.763638; batch adversarial loss: 3.725904\n",
      "epoch 1; iter: 14; batch classifier loss: 0.753987; batch adversarial loss: 3.720403\n",
      "epoch 1; iter: 15; batch classifier loss: 0.731780; batch adversarial loss: 3.709384\n",
      "epoch 1; iter: 16; batch classifier loss: 0.776539; batch adversarial loss: 3.714598\n",
      "epoch 1; iter: 17; batch classifier loss: 0.743523; batch adversarial loss: 3.729327\n",
      "epoch 1; iter: 18; batch classifier loss: 0.700741; batch adversarial loss: 3.721736\n",
      "epoch 1; iter: 19; batch classifier loss: 0.632679; batch adversarial loss: 3.724268\n",
      "epoch 1; iter: 20; batch classifier loss: 0.706350; batch adversarial loss: 3.709811\n",
      "epoch 1; iter: 21; batch classifier loss: 0.683652; batch adversarial loss: 3.716700\n",
      "epoch 1; iter: 22; batch classifier loss: 0.672041; batch adversarial loss: 3.719959\n",
      "epoch 1; iter: 23; batch classifier loss: 0.662070; batch adversarial loss: 3.711993\n",
      "epoch 1; iter: 24; batch classifier loss: 0.630526; batch adversarial loss: 3.728816\n",
      "epoch 1; iter: 25; batch classifier loss: 0.574854; batch adversarial loss: 3.728230\n",
      "epoch 1; iter: 26; batch classifier loss: 0.562681; batch adversarial loss: 3.742302\n",
      "epoch 1; iter: 27; batch classifier loss: 0.540649; batch adversarial loss: 3.716567\n",
      "epoch 1; iter: 28; batch classifier loss: 0.576717; batch adversarial loss: 3.715358\n",
      "epoch 1; iter: 29; batch classifier loss: 0.503479; batch adversarial loss: 3.725264\n",
      "epoch 1; iter: 30; batch classifier loss: 0.556868; batch adversarial loss: 3.727194\n",
      "epoch 2; iter: 0; batch classifier loss: 0.537611; batch adversarial loss: 3.713457\n",
      "epoch 2; iter: 1; batch classifier loss: 0.509668; batch adversarial loss: 3.730114\n",
      "epoch 2; iter: 2; batch classifier loss: 0.476044; batch adversarial loss: 3.721471\n",
      "epoch 2; iter: 3; batch classifier loss: 0.522587; batch adversarial loss: 3.727117\n",
      "epoch 2; iter: 4; batch classifier loss: 0.490088; batch adversarial loss: 3.721257\n",
      "epoch 2; iter: 5; batch classifier loss: 0.465905; batch adversarial loss: 3.740016\n",
      "epoch 2; iter: 6; batch classifier loss: 0.447686; batch adversarial loss: 3.729166\n",
      "epoch 2; iter: 7; batch classifier loss: 0.458686; batch adversarial loss: 3.725034\n",
      "epoch 2; iter: 8; batch classifier loss: 0.430064; batch adversarial loss: 3.741273\n",
      "epoch 2; iter: 9; batch classifier loss: 0.425212; batch adversarial loss: 3.732696\n",
      "epoch 2; iter: 10; batch classifier loss: 0.405280; batch adversarial loss: 3.734439\n",
      "epoch 2; iter: 11; batch classifier loss: 0.423221; batch adversarial loss: 3.728695\n",
      "epoch 2; iter: 12; batch classifier loss: 0.426862; batch adversarial loss: 3.707139\n",
      "epoch 2; iter: 13; batch classifier loss: 0.410972; batch adversarial loss: 3.722204\n",
      "epoch 2; iter: 14; batch classifier loss: 0.394498; batch adversarial loss: 3.713705\n",
      "epoch 2; iter: 15; batch classifier loss: 0.413409; batch adversarial loss: 3.737690\n",
      "epoch 2; iter: 16; batch classifier loss: 0.414758; batch adversarial loss: 3.726439\n",
      "epoch 2; iter: 17; batch classifier loss: 0.399764; batch adversarial loss: 3.715286\n",
      "epoch 2; iter: 18; batch classifier loss: 0.342232; batch adversarial loss: 3.741620\n",
      "epoch 2; iter: 19; batch classifier loss: 0.392566; batch adversarial loss: 3.739787\n",
      "epoch 2; iter: 20; batch classifier loss: 0.389585; batch adversarial loss: 3.715882\n",
      "epoch 2; iter: 21; batch classifier loss: 0.378386; batch adversarial loss: 3.728272\n",
      "epoch 2; iter: 22; batch classifier loss: 0.392694; batch adversarial loss: 3.724486\n",
      "epoch 2; iter: 23; batch classifier loss: 0.364721; batch adversarial loss: 3.715400\n",
      "epoch 2; iter: 24; batch classifier loss: 0.363144; batch adversarial loss: 3.718371\n",
      "epoch 2; iter: 25; batch classifier loss: 0.326667; batch adversarial loss: 3.734735\n",
      "epoch 2; iter: 26; batch classifier loss: 0.348976; batch adversarial loss: 3.718245\n",
      "epoch 2; iter: 27; batch classifier loss: 0.344584; batch adversarial loss: 3.722294\n",
      "epoch 2; iter: 28; batch classifier loss: 0.347387; batch adversarial loss: 3.725701\n",
      "epoch 2; iter: 29; batch classifier loss: 0.310200; batch adversarial loss: 3.722409\n",
      "epoch 2; iter: 30; batch classifier loss: 0.316144; batch adversarial loss: 3.739583\n",
      "epoch 3; iter: 0; batch classifier loss: 0.313580; batch adversarial loss: 3.734560\n",
      "epoch 3; iter: 1; batch classifier loss: 0.315343; batch adversarial loss: 3.734612\n",
      "epoch 3; iter: 2; batch classifier loss: 0.292728; batch adversarial loss: 3.741746\n",
      "epoch 3; iter: 3; batch classifier loss: 0.316134; batch adversarial loss: 3.730970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3; iter: 4; batch classifier loss: 0.306072; batch adversarial loss: 3.728918\n",
      "epoch 3; iter: 5; batch classifier loss: 0.307814; batch adversarial loss: 3.742266\n",
      "epoch 3; iter: 6; batch classifier loss: 0.313967; batch adversarial loss: 3.729022\n",
      "epoch 3; iter: 7; batch classifier loss: 0.299893; batch adversarial loss: 3.728607\n",
      "epoch 3; iter: 8; batch classifier loss: 0.287814; batch adversarial loss: 3.718416\n",
      "epoch 3; iter: 9; batch classifier loss: 0.287074; batch adversarial loss: 3.725401\n",
      "epoch 3; iter: 10; batch classifier loss: 0.279586; batch adversarial loss: 3.746905\n",
      "epoch 3; iter: 11; batch classifier loss: 0.273372; batch adversarial loss: 3.758242\n",
      "epoch 3; iter: 12; batch classifier loss: 0.268778; batch adversarial loss: 3.740175\n",
      "epoch 3; iter: 13; batch classifier loss: 0.271226; batch adversarial loss: 3.721240\n",
      "epoch 3; iter: 14; batch classifier loss: 0.253401; batch adversarial loss: 3.761651\n",
      "epoch 3; iter: 15; batch classifier loss: 0.261009; batch adversarial loss: 3.731069\n",
      "epoch 3; iter: 16; batch classifier loss: 0.279937; batch adversarial loss: 3.721155\n",
      "epoch 3; iter: 17; batch classifier loss: 0.254943; batch adversarial loss: 3.763981\n",
      "epoch 3; iter: 18; batch classifier loss: 0.255405; batch adversarial loss: 3.744275\n",
      "epoch 3; iter: 19; batch classifier loss: 0.271349; batch adversarial loss: 3.713373\n",
      "epoch 3; iter: 20; batch classifier loss: 0.262028; batch adversarial loss: 3.721984\n",
      "epoch 3; iter: 21; batch classifier loss: 0.269227; batch adversarial loss: 3.732561\n",
      "epoch 3; iter: 22; batch classifier loss: 0.248011; batch adversarial loss: 3.753270\n",
      "epoch 3; iter: 23; batch classifier loss: 0.227638; batch adversarial loss: 3.749408\n",
      "epoch 3; iter: 24; batch classifier loss: 0.237888; batch adversarial loss: 3.763052\n",
      "epoch 3; iter: 25; batch classifier loss: 0.229842; batch adversarial loss: 3.753456\n",
      "epoch 3; iter: 26; batch classifier loss: 0.241909; batch adversarial loss: 3.736616\n",
      "epoch 3; iter: 27; batch classifier loss: 0.227037; batch adversarial loss: 3.748862\n",
      "epoch 3; iter: 28; batch classifier loss: 0.229994; batch adversarial loss: 3.738607\n",
      "epoch 3; iter: 29; batch classifier loss: 0.214898; batch adversarial loss: 3.764717\n",
      "epoch 3; iter: 30; batch classifier loss: 0.239942; batch adversarial loss: 3.731864\n",
      "epoch 4; iter: 0; batch classifier loss: 0.215722; batch adversarial loss: 3.755016\n",
      "epoch 4; iter: 1; batch classifier loss: 0.235006; batch adversarial loss: 3.740727\n",
      "epoch 4; iter: 2; batch classifier loss: 0.223695; batch adversarial loss: 3.737964\n",
      "epoch 4; iter: 3; batch classifier loss: 0.219583; batch adversarial loss: 3.754513\n",
      "epoch 4; iter: 4; batch classifier loss: 0.206847; batch adversarial loss: 3.772835\n",
      "epoch 4; iter: 5; batch classifier loss: 0.225351; batch adversarial loss: 3.744326\n",
      "epoch 4; iter: 6; batch classifier loss: 0.194216; batch adversarial loss: 3.752586\n",
      "epoch 4; iter: 7; batch classifier loss: 0.203559; batch adversarial loss: 3.741700\n",
      "epoch 4; iter: 8; batch classifier loss: 0.218903; batch adversarial loss: 3.767123\n",
      "epoch 4; iter: 9; batch classifier loss: 0.192884; batch adversarial loss: 3.761532\n",
      "epoch 4; iter: 10; batch classifier loss: 0.210927; batch adversarial loss: 3.754419\n",
      "epoch 4; iter: 11; batch classifier loss: 0.189782; batch adversarial loss: 3.791569\n",
      "epoch 4; iter: 12; batch classifier loss: 0.213103; batch adversarial loss: 3.763950\n",
      "epoch 4; iter: 13; batch classifier loss: 0.202236; batch adversarial loss: 3.775052\n",
      "epoch 4; iter: 14; batch classifier loss: 0.175027; batch adversarial loss: 3.775881\n",
      "epoch 4; iter: 15; batch classifier loss: 0.213836; batch adversarial loss: 3.743286\n",
      "epoch 4; iter: 16; batch classifier loss: 0.202140; batch adversarial loss: 3.749633\n",
      "epoch 4; iter: 17; batch classifier loss: 0.200901; batch adversarial loss: 3.780682\n",
      "epoch 4; iter: 18; batch classifier loss: 0.190720; batch adversarial loss: 3.780779\n",
      "epoch 4; iter: 19; batch classifier loss: 0.199202; batch adversarial loss: 3.754910\n",
      "epoch 4; iter: 20; batch classifier loss: 0.178533; batch adversarial loss: 3.774585\n",
      "epoch 4; iter: 21; batch classifier loss: 0.175255; batch adversarial loss: 3.792412\n",
      "epoch 4; iter: 22; batch classifier loss: 0.187553; batch adversarial loss: 3.789207\n",
      "epoch 4; iter: 23; batch classifier loss: 0.192283; batch adversarial loss: 3.767899\n",
      "epoch 4; iter: 24; batch classifier loss: 0.178437; batch adversarial loss: 3.779642\n",
      "epoch 4; iter: 25; batch classifier loss: 0.192460; batch adversarial loss: 3.762065\n",
      "epoch 4; iter: 26; batch classifier loss: 0.189481; batch adversarial loss: 3.753385\n",
      "epoch 4; iter: 27; batch classifier loss: 0.174796; batch adversarial loss: 3.777086\n",
      "epoch 4; iter: 28; batch classifier loss: 0.167956; batch adversarial loss: 3.797268\n",
      "epoch 4; iter: 29; batch classifier loss: 0.164461; batch adversarial loss: 3.790265\n",
      "epoch 4; iter: 30; batch classifier loss: 0.172177; batch adversarial loss: 3.776642\n"
     ]
    }
   ],
   "source": [
    "ids_ph = tf.placeholder(tf.float32, shape=[32,128])\n",
    "masks_ph = tf.placeholder(tf.float32, shape=[32,128])\n",
    "sentenceIds_ph = tf.placeholder(tf.float32, shape=[32,128])\n",
    "\n",
    "gender_ph = tf.placeholder(tf.float32, shape=[32,128])\n",
    "ner_labels_ph = tf.placeholder(tf.float32, shape=[32,128])\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "\n",
    "protect_vars = [var for var in tf.trainable_variables() if 'gender' in var.name]\n",
    "pred_vars = model.layers[3]._trainable_weights + [var for var in tf.trainable_variables() if any(x in var.name for x in [\"pred_dense\",\"ner\"])]\n",
    "    \n",
    "y_pred = model([ids_ph, masks_ph, sentenceIds_ph], training=True)\n",
    "\n",
    "ner_loss = model_utils.custom_loss(ner_labels_ph, y_pred[\"ner\"])\n",
    "gender_loss = model_utils.custom_loss_protected(gender_ph, y_pred[\"gender\"])\n",
    "\n",
    "protect_opt = tf.train.AdamOptimizer(protect_learning_rate)\n",
    "pred_opt = tf.train.AdamOptimizer(pred_learning_rate)\n",
    "\n",
    "protect_grads = {var: grad for (grad, var) in protect_opt.compute_gradients(gender_loss,var_list=pred_vars)}\n",
    "pred_grads = []\n",
    "\n",
    "for (grad, var) in pred_opt.compute_gradients(ner_loss, var_list=pred_vars):\n",
    "    unit_protect = tf_normalize(protect_grads[var])\n",
    "    # the two lines below can be commented out to train without debiasing\n",
    "    grad -= tf.reduce_sum(grad * unit_protect) * unit_protect\n",
    "    grad -= tf.math.scalar_mul(protect_loss_weight, protect_grads[var])\n",
    "    pred_grads.append((grad, var))\n",
    "\n",
    "pred_min = pred_opt.apply_gradients(pred_grads, global_step=global_step)\n",
    "protect_min = protect_opt.minimize(gender_loss, var_list=[protect_vars], global_step=global_step)\n",
    "\n",
    "model_utils.initialize_vars(sess)\n",
    "\n",
    "# Begin training\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    shuffled_ids = np.random.choice(num_train_samples, num_train_samples)\n",
    "\n",
    "    for i in range(num_train_samples//32):\n",
    "\n",
    "        ids, masks, sentence_ids, ner_labels, gender_labels, race_labels = random_batch(train_data)\n",
    "\n",
    "        batch_feed_dict = {ids_ph: ids, \n",
    "                           masks_ph: masks,\n",
    "                           sentenceIds_ph: sentence_ids,\n",
    "                           gender_ph: gender_labels,\n",
    "                           ner_labels_ph: race_labels}\n",
    "\n",
    "\n",
    "        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = sess.run([\n",
    "            pred_min,\n",
    "            protect_min,\n",
    "            ner_loss,\n",
    "            gender_loss\n",
    "        ], feed_dict=batch_feed_dict)\n",
    "\n",
    "        #if i % 200 == 0:\n",
    "        print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                 pred_protected_attributes_loss_vale))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
