{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>A Simple Named Entity Recognition Model using  BERT  and Keras</center></h1>\n",
    "<h4><center>Initially prepared for UC Berkeley MIDS - W266</center></h4>\n",
    "\n",
    "\n",
    "<h3><center>SUMMARY</center></h3>\n",
    "\n",
    "In this notebook we investigate how we can leverage BERT (see [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"](https://arxiv.org/pdf/1810.04805.pdf), by Devlin/Chang/Lee/Toutanova, Google AI Language) for the problem of Named Entity Recognition (NER). Their paper actually contains the NER use case as a fine-tuning example, but we are not striving to replicate necessarily exactly their approach but build the model in the most approachable and 'naive' way, i.e. simply applying a straightforward model that follows intuitively the BERT mantra leveraging its context-based embeddings.\n",
    "\n",
    "\n",
    "We look at the effect of also fine-tuning BERT layers vs just adding and training classification layers on top of the BERT model and find that in this cursory and certainly incomplete study the re-training of BERT layers does offer some advantages. We also perform a test reducing the training data by 90% and find that the results are still quite decent, re-emphasizing BERT's usefulness in situations where the data set is on the small side.  \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Introduction & Approach](#ia)   \n",
    "1. [Introduction](#intro)  \n",
    "2. [Problem Definition & Metrics](#problem)  \n",
    "3. [Notebook Strategy](#strategy)  \n",
    "\n",
    "\n",
    "II. [Setup](#setup)   \n",
    "1. [Data](#data)  \n",
    "2. [BERT](#bert)  \n",
    "3. [Getting Started](#start)  \n",
    "\n",
    "\n",
    "III. [Data Preprocessing](#preprocess)   \n",
    "1. [BERT Tokenizer](#tokenizer)  \n",
    "2. [Extraction](#extract)  \n",
    "3. [Initial Data Analysis](#analysis) \n",
    "4. [Baseline: Always picking 'Other'](#baseline) \n",
    "5. [Train/Test Split](#split)  \n",
    "\n",
    "IV. [The Model](#model)   \n",
    "1. [Custom Loss & Accuracy](#custom)  \n",
    "2. [BERT Layer](#bert_layer)  \n",
    "2. [Model Construction](#ner_model) \n",
    "\n",
    "\n",
    "V. [Model Runs/Experiments](#runs)   \n",
    "1. [With BERT-layer Fine-Tuning](#retrain)  \n",
    "2. [Predictions & Confusion Matrix](#confusion)  \n",
    "3. [Without BERT-Layer Fine-Tuning](#basic)  \n",
    "4. [A 90%-Reduced Training Set](#tiny)  \n",
    "\n",
    "\n",
    "VI. [Summary](#summary)   \n",
    "\n",
    "\n",
    "\n",
    "## I. Introduction & Approach <a id=\"ia\" />\n",
    "\n",
    "### I. Introduction & Strategy <a id=\"intro\" />\n",
    "\n",
    "BERT and other context-aware embedding frameworks like [ELMO](https://arxiv.org/abs/1802.05365), OpenAI's [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), and [XLNet](https://arxiv.org/pdf/1906.08237.pdf), etc.  -  provide extremely useful basis for many NLP tasks. A key reason why these frameworks are so useful is that they allow us to use the power of extensive pre-training that was done on a (set of) large corpus/era. \n",
    "\n",
    "More specifically, their ability to encode deep contextual relationships between words (and sentences or sentence segments) derived from a generic set of tasks provides us with the context-specific embeddings.  Depending on the task, we can then simply add a couple of classification layers with a modest number of weights to fine-tune the combined model to our very specific NLP task that may not have the luxury of a very large labeled data set.\n",
    "\n",
    "There are numerous and very good resources available on the web for various of these tasks (Movie Reviews, Sentiment  Analysis, etc,.). In this notebook, we want to consider the task of Named Entity Recognition, as it features a number of useful complexities that are good to discuss:\n",
    "\n",
    "* token-level vs. sentence-level BERT output, which seems to be less-often discussed \n",
    "* potential one-to-many split of word-to-token by the tokenizer (what are we going to do for the labels?) \n",
    "* potentially a need for custom loss and accuracy definitions\n",
    "\n",
    "Conceptually, we will follow the original BERT paper in its approach to NER:\n",
    "\n",
    "<img src=\"BERT_NER_Devlin_et_al.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<center>Image Source: \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"</center>\n",
    "\n",
    "Each word will need to be tokenized. We will then sentence-by-sentence feed in the tokenized text into BERT, resulting in our case (we are using the BERT's base model) in a 768-dimensional output vector for each input token (and other tokens that BERT wants us to add). We then simply add a fully-connected hidden layer and finally a classification of suitable dimension that will take each token-output and make a decision on its NER label.  \n",
    "\n",
    "\n",
    "This is very intuitive. However, a few obvious questions arise that we want to look at in this notebook:\n",
    "\n",
    "1) How do we need to pre-process the data set to be suitable for BERT?   \n",
    "3) How can we build the model in Keras?   \n",
    "4) How can we incorporate custom loss functions and accuracy calculations?   \n",
    "5) What does fine-tuning mean? Is it just adding and training new layer(s), or do I re-train BERT layers as well?   \n",
    "6) Do I need to worry about customizing the optimizer?  \n",
    "\n",
    "These and other questions we hope to be able to shed some light on. The dataset we will be using is the \"**Annotated Corpus for Named Entity Recognition using GMB [Groningen Meaning Bank]**\", which is shared on [Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). It contains sentences with 1m+ words, conveniently annotated with POS and NER tags. \n",
    "\n",
    "### I.2. Problem Definition & Metrics <a id=\"problem\" />\n",
    "\n",
    "Our task will obviously be to properly identify the NER tags. There are potentially two things to consider when we want to define our success metrics: \n",
    "\n",
    "1) If we send the data sentence-by-sentence, we will need to apply padding to ensure consistent length. This will create 'new' labels that we will call 'nerPad'. Also, words can be split into multiple tokens, requiring us to add filler labels, which we will denote 'nerX'.  (There are multiple ways to address this, but this is what we are choosing here.)\n",
    "\n",
    "If we do that, should we look at accuracy over all tokens? Probably not. So our **first metric will be: accuracy for tokens which were part of the original text (and only the first token if a word is split)**.\n",
    "\n",
    "2) As we know from NER problems, most tokens will be 'Other'. So we may get already a pretty decent baseline result by always predicting 'Other'. In situations like that it may be useful to also look at **our second metric: the accuracy for all original tokens that are not 'Other'**.\n",
    "\n",
    "\n",
    "### I.3. Notebook Strategy <a id=\"strategy\" />\n",
    "\n",
    "The outline that we will follow in this notebook is this:\n",
    "\n",
    "**1) Process the text**\n",
    "* re-assemble words into sentences. (The corpus is of the form one-line-one-word, with sentence markers.)\n",
    "* tokenize the sentences with the BERT Tokenizer\n",
    "* create the input ids required for BERT:\n",
    "   * **sentence_ids** [the list of token ids for each sentence]\n",
    "   * **mask_ids** [the specification whether a specific token should be masked out (we mask out '[PAD]' tokens)]\n",
    "   * **sequence_ids** [used to denote whether a token is part of the first, second, or other segment in each input example. For us, this will always be '0'.]\n",
    "* prepare the labels  \n",
    "\n",
    "Some complications can include:\n",
    "- some words may not be in vocab\n",
    "- words can get split into multiple tokens. What are the labels?\n",
    "- Sentences do not all have the same length. Padding!\n",
    "- usual formatting details, like \"10,000\" and \"\"\" for quotes in this case.\n",
    "\n",
    "**2) Analyze and prepare the data**\n",
    "* Identify balance/imbalance situation\n",
    "* Estimate baseline accuracy defined by 'always picking the most common token'\n",
    "* Split into training and test set\n",
    "\n",
    "**3) Build the model**\n",
    "* Build BERT layer\n",
    "* Add classification layer(s)\n",
    "* Define custom loss functions and metrics\n",
    "\n",
    "**4) Run a few experiments**\n",
    "* Allow for re-training of a few BERT layers\n",
    "* Investigate the confusion matrix\n",
    "* Compare results with the model without re-training BERT layers\n",
    "* Test how good the results would be if you only at 10% of the training data (~4k sentences) \n",
    "\n",
    "\n",
    "This notebook leverages (obviously) BERT, a Also, a simplified/narrowed version of the Keras BERT layer implementation of Jacob Zweig's blog post [\"BERT in Keras with Tensorflow hub\"](https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b) is used.  \n",
    "\n",
    "The notebook was run with **Tensorflow 1.14** leveraging one GPU with 4 GB of memory. (Tensorflow 2.0 is not playing well yet with the local OS/CUDA setup.). In a future version of this notebook we plan to adjust it also for a cloud environment and Tensorflow 2.0.\n",
    "\n",
    "\n",
    "\n",
    "## II. Setup & Strategy\n",
    "\n",
    "### II.1. Data<a id=\"data\" />\n",
    "\n",
    "First, obtain the dataset (\"**Annotated Corpus for Named Entity Recognition using GMB [Groningen Meaning Bank]**\", which is shared on [Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus). The ner_dataset.csv file is the relevant file.\n",
    "\n",
    "Let us take a quick peek at the file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",.,.,O\n",
      "Sentence: 47958,They,PRP,O\n",
      ",say,VBP,O\n",
      ",not,RB,O\n",
      ",all,DT,O\n",
      ",of,IN,O\n",
      ",the,DT,O\n",
      ",rockets,NNS,O\n",
      ",exploded,VBD,O\n",
      ",upon,IN,O\n",
      ",impact,NN,O\n",
      ",.,.,O\n",
      "Sentence: 47959,Indian,JJ,B-gpe\n",
      ",forces,NNS,O\n",
      ",said,VBD,O\n",
      ",they,PRP,O\n",
      ",responded,VBD,O\n",
      ",to,TO,O\n",
      ",the,DT,O\n",
      ",attack,NN,O\n"
     ]
    }
   ],
   "source": [
    "!tail -20 'ner_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the words line-by-line, the labels (POS and NER), and the sentence boundaries. Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. BERT<a id=\"bert\" />\n",
    "\n",
    "Next, with your favorite approach, clone the [BERT](https://github.com/google-research/bert) Github repo and record your local bert-master path. The path needs to be made known to Jupyter, which we will do two cells down. \n",
    "\n",
    "We will also use BERT base from Tensorflow Hub (the **cased** BASE model.)\n",
    "\n",
    "\n",
    "### II.3. Getting Started<a id=\"start\" />\n",
    "\n",
    "We start with some imports, adding the data and bert path, and then completing all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 13:17:14.146509 139635640141632 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path =   '<your local BERT path>' # change as needed\n",
    "data_path = '<your data path>'  # path to ner_dataset.csv file , from \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)\n",
    "sys.path.insert(0,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some key hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we will need to do quite a bit of pre-processing. BERT - as well as NER in general - requires us to process the text in a larger context, which suggests that we should send the data to BERT sentence-by-sentence. (An alternative would also be to just chunk up the text, irrespective of sentence boundaries.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Preprocessing <a id=\"preprocess\" />\n",
    "\n",
    "### III.1 BERT Tokenizer<a id=\"tokenizer\" />\n",
    "\n",
    "We first start by defining and exploring the BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 13:17:15.611844 139635640141632 deprecation.py:323] From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0727 13:17:17.479922 139635640141632 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the tokenizer. You will see that the tokenizer occasionally splits one word into multiple tokens. Why is that the case? Because the approach of using word pieces reduces the vocabulary size and/or number of unknown words.\n",
    "\n",
    "Here is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'll', 'learn', 'to', 'swim', 'in', '123', '##42', 'years', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('I\\'ll learn to swim in 12342 years.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the \"I'll\" phrase and the number '12342' got split. This already highlights an area one needs to address: splitting of tokens will need to be accounted for in the labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 178, 112, 1325, 3858, 1106, 11231, 1107, 13414, 23117, 1201, 119]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([\n",
    "    '[CLS]', 'i', \"'\",'ll', 'learn','to','swim','in','123', '##42', 'years', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Faye']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([20958])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now we are ready to use it for our text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2. Extraction<a id=\"extract\"/>\n",
    "\n",
    "\n",
    "We have seen above how the data set looks. We can now turn to the pre-processing and creating the input for BERT. Specifically, we need to:\n",
    "\n",
    "1) Tokenize the sentences. Note again that one word can be split into multiple tokens, and we need to insert custom labels when that happens to make sure we don't mess up the alignment. We choose a new 'nerX' label here.\n",
    "\n",
    "2) Create BERT tokens and add [CLS], [PAD], etc.\n",
    "\n",
    "3) Convert these tokens into ids, also via the tokenizer. These qill create the sentence_ids.\n",
    "\n",
    "4) Create the mask ids. Mask out all of the padding tokens.\n",
    "\n",
    "5) Create the sequence ids. In our case, they are all '0' as we do not compare or even have multiple sentences in one example.\n",
    "\n",
    "To do this, we first define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word, pos, ner):\n",
    "    \"\"\"\n",
    "    Convert a word into a word token and add supplied NER and POS labels. Note that the word can be  \n",
    "    tokenized to two or more tokens. Correspondingly, we add - for now - custom 'X' tokens to the labels in order to \n",
    "    maintain the 1:1 mappings between word tokens and labels.\n",
    "    \n",
    "    arguments: word, pos label, ner label\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['posToken'] = [pos] + ['posX'] * (tokenLength - 1)\n",
    "    addDict['nerToken'] = [ner] + ['nerX'] * (tokenLength - 1)\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    \n",
    "    return addDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['protest'],\n",
       " 'posToken': ['VB'],\n",
       " 'nerToken': ['O'],\n",
       " 'tokenLength': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('protest', 'VB', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['Iraq'],\n",
       " 'posToken': ['NNP'],\n",
       " 'nerToken': ['B-geo'],\n",
       " 'tokenLength': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('Iraq', 'NNP', 'B-geo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wordToken': ['1000', '##0'],\n",
       " 'posToken': ['CD', 'posX'],\n",
       " 'nerToken': ['O', 'nerX'],\n",
       " 'tokenLength': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addWord('10000', 'CD', 'O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to convert the text file into appropriate arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the file line by line and construct sentences. A sentence end is marked by the word 'sentence' in the next row.\n",
    "You need to take care of that. Also, you need to cap sentence length using max_length. Sentences which are shorter than \n",
    "max_length need to be padded. Also, we choose to end all sentences with a [SEP] token, padded or not. \n",
    "\"\"\"\n",
    "\n",
    "with io.open(data_path + 'ner_dataset.csv', 'r', encoding='utf-8', errors='ignore') as train:\n",
    "    text = train.readlines()\n",
    "\n",
    "\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "posTokenList = []\n",
    "nerTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "sentence = ''\n",
    "\n",
    "# always start with [CLS] tokens\n",
    "sentenceTokens = ['[CLS]']\n",
    "posTokens = ['[posCLS]']\n",
    "nerTokens = ['[nerCLS]']\n",
    "\n",
    "for line in text:\n",
    "    \n",
    "    cleanLine = re.sub(r'(?!(([^\"]*\"){2})*[^\"]*$),', '', line)  # deal with '\"10,000\"' and convert them to '10000' \n",
    "\n",
    "    sent, word, pos, ner = cleanLine.split(',')\n",
    "    \n",
    "    ner = ner[:-1]   # remove DOS token\n",
    "    \n",
    "    # if new sentence starts\n",
    "    if (sent[:8] == 'Sentence'):            \n",
    "            \n",
    "        sentenceLength = min(max_length -1, len(sentenceTokens))\n",
    "        sentLengthList.append(sentenceLength)\n",
    "        \n",
    "                    \n",
    "        # Create space for at least a final '[SEP]' token\n",
    "        if sentenceLength >= max_length - 1: \n",
    "            sentenceTokens = sentenceTokens[:max_length - 2]\n",
    "            posTokens = posTokens[:max_length - 2]\n",
    "            nerTokens = nerTokens[:max_length - 2]\n",
    "\n",
    "        # add a ['SEP'] token and padding\n",
    "        \n",
    "        sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "        \n",
    "        posTokens += ['[posSEP]'] + ['[posPAD]'] * (max_length - 1 - len(posTokens) )\n",
    "        nerTokens += ['[nerSEP]'] + ['[nerPAD]'] * (max_length - 1 - len(nerTokens) )\n",
    "            \n",
    "        sentenceList.append(sentence)\n",
    "\n",
    "        sentenceTokenList.append(sentenceTokens)\n",
    "\n",
    "        bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "        bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length -1 - sentenceLength ))\n",
    "        bertSequenceIDs.append([0] * (max_length))\n",
    "                             \n",
    "        posTokenList.append(posTokens)\n",
    "        nerTokenList.append(nerTokens)\n",
    "        \n",
    "        sentence = ''\n",
    "        sentenceTokens = ['[CLS]']\n",
    "        posTokens = ['[posCLS]']\n",
    "        nerTokens = ['[nerCLS]']\n",
    "        \n",
    "        sentence += ' ' + word\n",
    "\n",
    "    addDict = addWord(word, pos, ner)\n",
    "\n",
    "    sentenceTokens += addDict['wordToken']\n",
    "    posTokens += addDict['posToken']\n",
    "    nerTokens += addDict['nerToken']\n",
    "\n",
    "# The first two list elements need to be removed. 1st line in file is a-typical, and 2nd line does not end a sentence   \n",
    "sentLengthList = sentLengthList[2:]\n",
    "sentenceTokenList = sentenceTokenList[2:]\n",
    "bertSentenceIDs = bertSentenceIDs[2:]\n",
    "bertMasks = bertMasks[2:]\n",
    "bertSequenceIDs = bertSequenceIDs[2:]\n",
    "posTokenList = posTokenList[2:]\n",
    "nerTokenList = nerTokenList[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'They', 'marched', 'from', 'the', 'Houses', 'of', 'Parliament', 'to', 'a', 'rally', 'in', 'Hyde', 'Park', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(sentenceTokenList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[nerCLS]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'I-geo', 'O', '[nerSEP]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]']\n"
     ]
    }
   ],
   "source": [
    "print(nerTokenList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bertMasks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. Everything past the '[SEP]' token, i.e., the '[nerSEP]' label, is masked out. Also the sequence_ids are correct: there is only one sentence, so all ids should have the same value of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(bertSequenceIDs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable. \n",
    "\n",
    "\n",
    "### III.3. Initial Data Analysis<a id=\"analysis\" />\n",
    "\n",
    "It is important to understand the dataset prior to doing any modeling or training. First, what are the length of the original sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.0000e+00, 2.0000e+00, 3.0000e+00, 1.9000e+01, 1.0600e+02,\n",
       "        1.8400e+02, 3.0800e+02, 4.3000e+02, 5.1900e+02, 6.6700e+02,\n",
       "        8.2800e+02, 9.4000e+02, 1.0040e+03, 1.1560e+03, 1.2590e+03,\n",
       "        1.3620e+03, 1.4480e+03, 1.5610e+03, 1.5710e+03, 1.7080e+03,\n",
       "        1.7600e+03, 1.8350e+03, 1.9350e+03, 1.9450e+03, 1.9120e+03,\n",
       "        1.8550e+03, 1.9040e+03, 1.9736e+04]),\n",
       " array([ 2.        ,  2.96428571,  3.92857143,  4.89285714,  5.85714286,\n",
       "         6.82142857,  7.78571429,  8.75      ,  9.71428571, 10.67857143,\n",
       "        11.64285714, 12.60714286, 13.57142857, 14.53571429, 15.5       ,\n",
       "        16.46428571, 17.42857143, 18.39285714, 19.35714286, 20.32142857,\n",
       "        21.28571429, 22.25      , 23.21428571, 24.17857143, 25.14285714,\n",
       "        26.10714286, 27.07142857, 28.03571429, 29.        ]),\n",
       " <a list of 28 Patch objects>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentenceLengths= [l for l in sentLengthList]\n",
    "\n",
    "plt.hist(np.array(sentenceLengths), bins=(max_length-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An average sentence length of ~25 (incl. extra tokens!) is roughly expected. It turns out that on these types of corpora an average sentence length of ~20 tends to be seen. The big spike on the right obviously corresponds to all sentences that we had to truncate. \n",
    "\n",
    "Next, we analyze the distribution of ner labels. First, we assign numbers to the labels and look at the overall distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bertSentenceIDs)\n",
    "\n",
    "nerClasses = pd.DataFrame(np.array(nerTokenList).reshape(-1))\n",
    "nerClasses.columns = ['tag']\n",
    "nerClasses.tag = pd.Categorical(nerClasses.tag)\n",
    "nerClasses['cat'] = nerClasses.tag.cat.codes\n",
    "nerClasses['sym'] = nerClasses.tag.cat.codes\n",
    "nerLabels = np.array(nerClasses.cat).reshape(numSentences, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7efdf4a16c88>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nerClasses[['cat']].hist(bins=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a lot of tables with value 16+... Let's see which labels these label numbers corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>occurences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-art</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-eve</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-geo</td>\n",
       "      <td>2</td>\n",
       "      <td>32376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-gpe</td>\n",
       "      <td>3</td>\n",
       "      <td>14570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-nat</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-org</td>\n",
       "      <td>5</td>\n",
       "      <td>18358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-per</td>\n",
       "      <td>6</td>\n",
       "      <td>16018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-tim</td>\n",
       "      <td>7</td>\n",
       "      <td>17657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-art</td>\n",
       "      <td>8</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-eve</td>\n",
       "      <td>9</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-geo</td>\n",
       "      <td>10</td>\n",
       "      <td>6037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-gpe</td>\n",
       "      <td>11</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-nat</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-org</td>\n",
       "      <td>13</td>\n",
       "      <td>14829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-per</td>\n",
       "      <td>14</td>\n",
       "      <td>15950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-tim</td>\n",
       "      <td>15</td>\n",
       "      <td>5091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>16</td>\n",
       "      <td>765432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[nerCLS]</td>\n",
       "      <td>17</td>\n",
       "      <td>47958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[nerPAD]</td>\n",
       "      <td>18</td>\n",
       "      <td>268308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[nerSEP]</td>\n",
       "      <td>19</td>\n",
       "      <td>47958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nerX</td>\n",
       "      <td>20</td>\n",
       "      <td>166746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tag  cat  occurences\n",
       "0      B-art    0         345\n",
       "1      B-eve    1         255\n",
       "2      B-geo    2       32376\n",
       "3      B-gpe    3       14570\n",
       "4      B-nat    4         167\n",
       "5      B-org    5       18358\n",
       "6      B-per    6       16018\n",
       "7      B-tim    7       17657\n",
       "8      I-art    8         258\n",
       "9      I-eve    9         207\n",
       "10     I-geo   10        6037\n",
       "11     I-gpe   11         181\n",
       "12     I-nat   12          39\n",
       "13     I-org   13       14829\n",
       "14     I-per   14       15950\n",
       "15     I-tim   15        5091\n",
       "16         O   16      765432\n",
       "17  [nerCLS]   17       47958\n",
       "18  [nerPAD]   18      268308\n",
       "19  [nerSEP]   19       47958\n",
       "20      nerX   20      166746"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerDistribution = (nerClasses.groupby(['tag', 'cat']).agg({'sym':'count'}).reset_index()\n",
    "                   .rename(columns={'sym':'occurences'}))\n",
    "\n",
    "numNerClasses = nerDistribution.tag.nunique()\n",
    "\n",
    "nerDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. 16 corresponds to 'O', and all 'extension' labels (i.e., those that were not part of the original data) occur at 17+. \n",
    "\n",
    "'O' is the most common token - by far.\n",
    "\n",
    "### III.4. Baseline: Always picking 'Other'<a id=\"baseline\" />\n",
    "\n",
    "Let's see what a baseline would give for the actual text tokens, if I ALWAYS chose the most common token 'O':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16    0.8432\n",
       "Name: occurences, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_occurences = nerDistribution.loc[nerDistribution.tag == 'O','occurences']\n",
    "All_occurences = nerDistribution[nerDistribution.cat < 17]['occurences'].sum()\n",
    "\n",
    "O_occurences/All_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **84.4%** is the baseline to beat for our first metric! Can we do that? We'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.5. Train/Test Split and Final Data Preparation<a id=\"split\" />\n",
    "\n",
    "In the last step we need to prepare both labels and input for the model, including the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split - in a pretty manual way - the examples into a train and test set. We create a random binary value for each sentence that we use to split train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "nerLabels_train =[]\n",
    "nerLabels_test = []\n",
    "\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_train.append(nerLabels[example])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        nerLabels_test.append(nerLabels[example])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "nerLabels_train = np.array(nerLabels_train)\n",
    "nerLabels_test = np.array(nerLabels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 26159,  1104,  8568,  4487,  5067,  1138,  9639,  1194,\n",
       "        1498,  1106,  5641,  1103,  1594,  1107,  5008,  1105,  4555,\n",
       "        1103, 10602,  1104,  1418,  2830,  1121,  1115,  1583,   119,\n",
       "         102,     0,     0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 16, 16, 16, 20, 20, 16, 16, 16,  2, 16, 16, 16, 16, 16,  2, 16,\n",
       "       16, 16, 16, 16,  3, 16, 16, 16, 16, 16, 19, 18, 18], dtype=int8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerLabels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Thousands', 'of', 'demons', '##tra', '##tors', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(sentenceTokenList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a few train/test positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step, we prepare the actual train and test input and label data. For convenience (quick functionality test on small data set), we introduce parameters k_start & k_end to just use a slide of the full dataset. (Setting k_end to -1 corresponds to using the whole set (as we will do in the following). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "\n",
    "\n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "\n",
    "labels_train_k = nerLabels_train[k_start:k_end_train]\n",
    "labels_test_k = nerLabels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We are all set to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. The Model<a id=\"model\"/>\n",
    "\n",
    "### IV.1. Custom Loss & Accuracy<a id=\"custom\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a **custom loss function** because we only want to optimize for the labels that we actually had in the text, not the extra ones like '[nerPAD]', etc. Our cost function is therefore derived from sparse_categorical_crossentropy, but we choose to modify the function a bit:  we want to mask out all tokens that have a token id larger or equal of 17, corresponding to the extra tokens:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 17)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, numNerClasses])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work as advertised? Let's create a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([[17],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5108274\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[17],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.0,0,0,0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(custom_loss(y_true, y_pred).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the manual calculation of $-\\log((y^1_{pred})_0)$ (remember that $y^0$ is masked out because the true label is 17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5108256237659907"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is correct! The position where the true label is 17 is ignored because of the mask!\n",
    "\n",
    "In a similar vein, we define and test a **custom accuracy** calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 17)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also define another accuracy calculation that only looks at the non-Other labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 16)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numNerClasses]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[17],[0]])\n",
    "\n",
    "y_pred = tf.constant([\n",
    "    [0.6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,.4,0,0,0],\n",
    "    [0.6,0.4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "])\n",
    "\n",
    "\n",
    "print(custom_acc_orig_tokens(y_true, y_pred).eval())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again... correct! The false value for the '17' example is not considered.\n",
    "\n",
    "Let's close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, define an Adam optimizer with new learning rate and beta parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2 Keras BERT Layer<a id=\"bert_layer\"/>\n",
    "\n",
    "\n",
    "We start by creating a Keras layer for the BERT module from Tensorflow hub. (Note: This code is simply a more specified and for our purposes adjusted version of the equivalent cell in (https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b)   \n",
    "\n",
    "Note, how then trainable variables are built. (See also: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the summary statistics for TensorBoard. And then we can construct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3 Model Construction<a id=\"ner_model\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build the model! Let's be pretty simple. No drop-out etc for now. But we re-train three BERT layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    print(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(21, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=losses, optimizer=optimizer, metrics=[custom_acc_orig_tokens, \n",
    "                                                          custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## V. Model Runs/Experiments<a id=\"runs\"/>\n",
    "\n",
    "### V.1. With BERT-Layer Re-Training<a id=\"retrain\"/>\n",
    "\n",
    "It is time to run the first test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0727 13:17:54.123116 139635640141632 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer/bert_layer_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
      "WARNING:tensorflow:From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 13:17:54.288343 139635640141632 deprecation.py:506] From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  Tensor(\"ner/truediv:0\", shape=(?, ?, 21), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 21)     5397        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,133,657\n",
      "Trainable params: 28,553,749\n",
      "Non-trainable params: 80,579,908\n",
      "__________________________________________________________________________________________________\n",
      "Train on 33690 samples, validate on 14268 samples\n",
      "WARNING:tensorflow:From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 13:17:56.382097 139635640141632 deprecation.py:323] From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0727 13:17:56.438941 139635640141632 deprecation.py:323] From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "33690/33690 [==============================] - 195s 6ms/sample - loss: 0.1426 - custom_acc_orig_tokens: 0.9592 - custom_acc_orig_non_other_tokens: 0.7892 - val_loss: 0.1136 - val_custom_acc_orig_tokens: 0.9657 - val_custom_acc_orig_non_other_tokens: 0.8146\n",
      "Epoch 2/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.1035 - custom_acc_orig_tokens: 0.9684 - custom_acc_orig_non_other_tokens: 0.8301 - val_loss: 0.1087 - val_custom_acc_orig_tokens: 0.9671 - val_custom_acc_orig_non_other_tokens: 0.8198\n",
      "Epoch 3/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0953 - custom_acc_orig_tokens: 0.9705 - custom_acc_orig_non_other_tokens: 0.8414 - val_loss: 0.1068 - val_custom_acc_orig_tokens: 0.9674 - val_custom_acc_orig_non_other_tokens: 0.8256\n",
      "Epoch 4/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0903 - custom_acc_orig_tokens: 0.9718 - custom_acc_orig_non_other_tokens: 0.8482 - val_loss: 0.1059 - val_custom_acc_orig_tokens: 0.9676 - val_custom_acc_orig_non_other_tokens: 0.8272\n",
      "Epoch 5/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0867 - custom_acc_orig_tokens: 0.9728 - custom_acc_orig_non_other_tokens: 0.8538 - val_loss: 0.1052 - val_custom_acc_orig_tokens: 0.9678 - val_custom_acc_orig_non_other_tokens: 0.8275\n",
      "Epoch 6/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0837 - custom_acc_orig_tokens: 0.9737 - custom_acc_orig_non_other_tokens: 0.8580 - val_loss: 0.1048 - val_custom_acc_orig_tokens: 0.9679 - val_custom_acc_orig_non_other_tokens: 0.8289\n",
      "Epoch 7/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0815 - custom_acc_orig_tokens: 0.9744 - custom_acc_orig_non_other_tokens: 0.8618 - val_loss: 0.1047 - val_custom_acc_orig_tokens: 0.9680 - val_custom_acc_orig_non_other_tokens: 0.8281\n",
      "Epoch 8/8\n",
      "33690/33690 [==============================] - 191s 6ms/sample - loss: 0.0798 - custom_acc_orig_tokens: 0.9746 - custom_acc_orig_non_other_tokens: 0.8627 - val_loss: 0.1048 - val_custom_acc_orig_tokens: 0.9681 - val_custom_acc_orig_non_other_tokens: 0.8296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdcd1e1c88>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = ner_model(max_length + 1, train_layers=4, optimizer = adam_customized)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k }),\n",
    "    epochs=8,\n",
    "    batch_size=32#,\n",
    "    #callbacks=[tensorboard]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**96.8% test accuracy for all original tokens and 83.0% for all original 'non-Other' tokens.... Not bad at all!!** And some tweaking and tuning should probably increase the values a bit more.\n",
    "\n",
    "Note that we used here the **Adam optimizer with custom values. Did that matter?** Why don't you try it...\n",
    "\n",
    "### V.2. Predictions & Confusion Matrix<a id=\"confusion\" />\n",
    "\n",
    "\n",
    "Let us look and see how well the model performs. We use the test here. (It probably would be better to split the data into train/validation/test, we are somewhat casual here).\n",
    "\n",
    "First, get all of the predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_infer = [X_test[0], X_test[1], X_test[2]]\n",
    "\n",
    "result = model.predict(\n",
    "    bert_inputs_infer, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14268, 30, 21)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct shape: # test sentences x sentence length x # classes. \n",
    "Let's get the prediction argmax for a random test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  2 16 16 16  6 14 14 14\n",
      " 14 16 16 16 16 16]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(result, axis=2)[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What were the labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 16 20 20 16 20 16 16 16 16 16 16 16 16 16 16  2 16 20 16  6 14 20 14\n",
      " 14 16 19 18 18 18]\n"
     ]
    }
   ],
   "source": [
    "print(nerLabels_test[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong? Correct!** Or.. is it?  \n",
    "\n",
    "**Question: Why are we not bothered by the first and the last 'mistakes', i.e., not identifying 20.17,19,18, etc.?**\n",
    "\n",
    "Let us now get the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_flat = [pred for preds in np.argmax(result, axis=2) for pred in preds]\n",
    "labels_flat = [label for labels in nerLabels_test for label in labels]\n",
    "\n",
    "clean_preds = []\n",
    "clean_labels = []\n",
    "\n",
    "for pred, label in zip(predictions_flat, labels_flat):\n",
    "    if label < 17:\n",
    "        clean_preds.append(pred)\n",
    "        clean_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 13:44:13.753916 139635640141632 deprecation.py:323] From /home/joachim/anaconda3/envs/tf18/lib/python3.6/site-packages/tensorflow/python/ops/confusion_matrix.py:193: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "cm = tf.math.confusion_matrix(\n",
    "    clean_labels,\n",
    "    clean_preds,\n",
    "    num_classes=None,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    name=None,\n",
    "    weights=None\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a little big and unbalanced to display. Let us focus on the rows/columns with the common labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     3,     36,  10296,   4098,     17,   4894,   4747,   4928,\n",
       "            5,     21,   1722,     23,      1,   4106,   5078,   1291,\n",
       "       228444])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8740     47    395    124     11     37     41    290]\n",
      " [   212   3951     50      2      0      6      2     27]\n",
      " [   670     41   3750    255     13    122     91    423]\n",
      " [   169      0    174   4006      2     43    180    178]\n",
      " [    90      0     18      3   4515      3      4    468]\n",
      " [    82      8    117     63      1   3216    285    374]\n",
      " [     7      0     19    177      0    138   4284     69]\n",
      " [   194     36    294    104    251    263    101 226138]]\n"
     ]
    }
   ],
   "source": [
    "cm_most = cm[[2,3,5,6,7,13,14,16],:] [:, [2,3,5,6,7,13,14,16]]\n",
    "\n",
    "print(cm_most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efdbc7fc208>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cm_most[:-1,:-1], cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.3 Without BERT-Layer Retraining (\"Did that Help?\")\n",
    "\n",
    "We will re-run the model, but without re-training of the top BERT layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0727 13:44:17.571316 139635640141632 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer_1/bert_layer_1_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
      "pred:  Tensor(\"ner_1/truediv:0\", shape=(?, ?, 21), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 21)     5397        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,133,657\n",
      "Trainable params: 202,261\n",
      "Non-trainable params: 108,931,396\n",
      "__________________________________________________________________________________________________\n",
      "Train on 33690 samples, validate on 14268 samples\n",
      "Epoch 1/8\n",
      "33690/33690 [==============================] - 123s 4ms/sample - loss: 0.1801 - custom_acc_orig_tokens: 0.9499 - custom_acc_orig_non_other_tokens: 0.7338 - val_loss: 0.1338 - val_custom_acc_orig_tokens: 0.9603 - val_custom_acc_orig_non_other_tokens: 0.7899\n",
      "Epoch 2/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.1263 - custom_acc_orig_tokens: 0.9621 - custom_acc_orig_non_other_tokens: 0.7990 - val_loss: 0.1232 - val_custom_acc_orig_tokens: 0.9631 - val_custom_acc_orig_non_other_tokens: 0.8055\n",
      "Epoch 3/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.1138 - custom_acc_orig_tokens: 0.9648 - custom_acc_orig_non_other_tokens: 0.8123 - val_loss: 0.1198 - val_custom_acc_orig_tokens: 0.9636 - val_custom_acc_orig_non_other_tokens: 0.8022\n",
      "Epoch 4/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.1057 - custom_acc_orig_tokens: 0.9668 - custom_acc_orig_non_other_tokens: 0.8221 - val_loss: 0.1181 - val_custom_acc_orig_tokens: 0.9644 - val_custom_acc_orig_non_other_tokens: 0.8055\n",
      "Epoch 5/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.0986 - custom_acc_orig_tokens: 0.9687 - custom_acc_orig_non_other_tokens: 0.8324 - val_loss: 0.1158 - val_custom_acc_orig_tokens: 0.9646 - val_custom_acc_orig_non_other_tokens: 0.8178\n",
      "Epoch 6/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.0921 - custom_acc_orig_tokens: 0.9705 - custom_acc_orig_non_other_tokens: 0.8421 - val_loss: 0.1157 - val_custom_acc_orig_tokens: 0.9647 - val_custom_acc_orig_non_other_tokens: 0.8096\n",
      "Epoch 7/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.0867 - custom_acc_orig_tokens: 0.9718 - custom_acc_orig_non_other_tokens: 0.8491 - val_loss: 0.1191 - val_custom_acc_orig_tokens: 0.9643 - val_custom_acc_orig_non_other_tokens: 0.8070\n",
      "Epoch 8/8\n",
      "33690/33690 [==============================] - 121s 4ms/sample - loss: 0.0817 - custom_acc_orig_tokens: 0.9730 - custom_acc_orig_non_other_tokens: 0.8558 - val_loss: 0.1171 - val_custom_acc_orig_tokens: 0.9653 - val_custom_acc_orig_non_other_tokens: 0.8147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd85de16d8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = ner_model(max_length + 1,train_layers=0,optimizer='adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"ner\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k}),\n",
    "    epochs=8,\n",
    "    batch_size=32#,\n",
    "    #callbacks=[tensorboard]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Close, but not quite as good.** - While one has to be careful given the different optimizer configurations and number of epochs, it looks as if not re-training BERT - in this case - increased the loss a bit. (It appears also that we over-trained a bit.) Let's call this **~96.5%/81.8%**, had we stopped a bit sooner.\n",
    "\n",
    "**Side Notes:** \n",
    " * Deeper re-training needs more compute resources\n",
    " * Deeper re-training often requires a tuned optimizer\n",
    " * Regularization is definitely important..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.4. A 90%-Reduced Training Set<a id=\"tiny\"/>\n",
    "\n",
    "\n",
    "The claim is that BERT is also very useful if one doesn't have much data. So let us see what happens if we cut the training data down to 10%. That leaves us with only ~3400 training examples. Not much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 33690, 30)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainSentences = 3370\n",
    "\n",
    "bert_inputs_train_tiny = [bert_inputs_train_k[0][:numTrainSentences,:], \\\n",
    "                          bert_inputs_train_k[1][:numTrainSentences,:], \\\n",
    "                          bert_inputs_train_k[2][:numTrainSentences,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_tiny = labels_train_k[:numTrainSentences,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0727 14:00:43.336763 139635640141632 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer_2/bert_layer_2_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
      "pred:  Tensor(\"ner_2/truediv:0\", shape=(?, ?, 21), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 21)     5397        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,133,657\n",
      "Trainable params: 202,261\n",
      "Non-trainable params: 108,931,396\n",
      "__________________________________________________________________________________________________\n",
      "Train on 3370 samples, validate on 14268 samples\n",
      "Epoch 1/8\n",
      "3370/3370 [==============================] - 45s 13ms/sample - loss: 0.4265 - custom_acc_orig_tokens: 0.8942 - custom_acc_orig_non_other_tokens: 0.4394 - val_loss: 0.2121 - val_custom_acc_orig_tokens: 0.9391 - val_custom_acc_orig_non_other_tokens: 0.6586\n",
      "Epoch 2/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1819 - custom_acc_orig_tokens: 0.9489 - custom_acc_orig_non_other_tokens: 0.7174 - val_loss: 0.1768 - val_custom_acc_orig_tokens: 0.9510 - val_custom_acc_orig_non_other_tokens: 0.7373\n",
      "Epoch 3/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1534 - custom_acc_orig_tokens: 0.9550 - custom_acc_orig_non_other_tokens: 0.7521 - val_loss: 0.1679 - val_custom_acc_orig_tokens: 0.9529 - val_custom_acc_orig_non_other_tokens: 0.7449\n",
      "Epoch 4/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1357 - custom_acc_orig_tokens: 0.9588 - custom_acc_orig_non_other_tokens: 0.7751 - val_loss: 0.1667 - val_custom_acc_orig_tokens: 0.9516 - val_custom_acc_orig_non_other_tokens: 0.7380\n",
      "Epoch 5/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1219 - custom_acc_orig_tokens: 0.9625 - custom_acc_orig_non_other_tokens: 0.7975 - val_loss: 0.1625 - val_custom_acc_orig_tokens: 0.9537 - val_custom_acc_orig_non_other_tokens: 0.7534\n",
      "Epoch 6/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1108 - custom_acc_orig_tokens: 0.9648 - custom_acc_orig_non_other_tokens: 0.8086 - val_loss: 0.1607 - val_custom_acc_orig_tokens: 0.9544 - val_custom_acc_orig_non_other_tokens: 0.7555\n",
      "Epoch 7/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.1017 - custom_acc_orig_tokens: 0.9680 - custom_acc_orig_non_other_tokens: 0.8245 - val_loss: 0.1602 - val_custom_acc_orig_tokens: 0.9552 - val_custom_acc_orig_non_other_tokens: 0.7578\n",
      "Epoch 8/8\n",
      "3370/3370 [==============================] - 43s 13ms/sample - loss: 0.0922 - custom_acc_orig_tokens: 0.9712 - custom_acc_orig_non_other_tokens: 0.8413 - val_loss: 0.1632 - val_custom_acc_orig_tokens: 0.9549 - val_custom_acc_orig_non_other_tokens: 0.7568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd7b6c8ac8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "model = ner_model(max_length + 1,train_layers=0,optimizer='adam')\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_tiny, \n",
    "    {\"ner\": labels_train_tiny },\n",
    "    validation_data=(bert_inputs_test_k, {\"ner\": labels_test_k}),\n",
    "    epochs=8,\n",
    "    batch_size=32#,\n",
    "    #callbacks=[tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, one would think! **~95.5%/75.7%** on the test set, compared to ~96.5%/81.8% on the full training set with 10x the data! So BERT is serving quite well for a smaller data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Summary<a id=\"summary\" />\n",
    "\n",
    "This finishes this cursory analysis of \"BERT for NER\". We pre-formatted our dataset, took care of tokenization and new inserted tokens (and labels!), defined a baseline model, and then - it would have been embarassing if we had failed - soundly beat the baseline with our Keras-based BERT+classification model. We saw that retraining of some BERT layers appeared to work well.  \n",
    "We also saw that even a small training set of about 3400 sentences did quite well using this architecture.\n",
    "\n",
    "All in all, we hope that this notebook was useful and despite its length reasonably readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
