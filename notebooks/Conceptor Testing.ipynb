{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 10)     2570        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,130,830\n",
      "Trainable params: 199,434\n",
      "Non-trainable params: 108,931,396\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "src_path =   '../src' # change as needed\n",
    "sys.path.insert(0,src_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import model_utils; reload(model_utils)\n",
    "\n",
    "max_length=10\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = model_utils.NER()\n",
    "    \n",
    "model.generate(max_length, train_layers=0, optimizer = \"adam\", debias=False)\n",
    "\n",
    "model_utils.initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from bert import tokenization\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\")\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        with tf.Session(config=config) as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"european\"))\n",
    "\n",
    "ids = token_ids + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "masks = [1 for i in range(len(token_ids))] + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "sequenceIDs = [0 for i in range(max_length)]\n",
    "\n",
    "for result in model.yieldBertEmbeddings([[ids],[masks],[sequenceIDs]]):\n",
    "    european_vector = np.sum(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"african\"))\n",
    "\n",
    "ids = token_ids + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "masks = [1 for i in range(len(token_ids))] + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "sequenceIDs = [0 for i in range(max_length)]\n",
    "\n",
    "for result in model.yieldBertEmbeddings([[ids],[masks],[sequenceIDs]]):\n",
    "    african_vector = np.sum(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"caucasian\"))\n",
    "\n",
    "ids = token_ids + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "masks = [1 for i in range(len(token_ids))] + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "sequenceIDs = [0 for i in range(max_length)]\n",
    "\n",
    "for result in model.yieldBertEmbeddings([[ids],[masks],[sequenceIDs]]):\n",
    "    caucasian_vector = np.sum(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"american\"))\n",
    "\n",
    "ids = token_ids + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "masks = [1 for i in range(len(token_ids))] + [0 for i in range(max_length - len(token_ids))]\n",
    "\n",
    "sequenceIDs = [0 for i in range(max_length)]\n",
    "\n",
    "for result in model.yieldBertEmbeddings([[ids],[masks],[sequenceIDs]]):\n",
    "    american_vector = np.sum(result, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2759936451911926"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial.distance.cosine(american_vector, african_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
